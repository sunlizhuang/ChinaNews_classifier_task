{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GDELT_Bert_tf.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOmwkkMENJh5UmSPQy+TBre"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"cxdPTbt3j2Qx","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsRgm0uejk9T","colab_type":"code","colab":{}},"source":["!pip install bert-tensorflow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qx_Ixlpujk_n","colab_type":"code","colab":{}},"source":["import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eho7_xVxjlCN","colab_type":"code","colab":{}},"source":["# Set the output directory for saving model file\n","# Optionally, set a GCP bucket location\n","\n","OUTPUT_DIR = 'OUTPUT_DIR_NAME'#@param {type:\"string\"}\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = False #@param {type:\"boolean\"}\n","BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wC5bPiSKjlFE","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","import os\n","import re\n","\n","# Load all files from a directory in a DataFrame.\n","def load_directory_data(directory):\n","  data = {}\n","  data[\"sentence\"] = []\n","  data[\"sentiment\"] = []\n","  for file_path in os.listdir(directory):\n","    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","      data[\"sentence\"].append(f.read())\n","      data[\"sentiment\"].append(re.match(\"\\d+--(\\d+)\\.txt\", file_path))\n","  return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","  pos_df[\"polarity\"] = 1\n","  neg_df[\"polarity\"] = 0\n","  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  dataset = tf.keras.utils.get_file(\n","      fname=\"chinaNews.zip\", \n","      origin=\"https://raw.githubusercontent.com/sunlizhuang/GDELT_chinaNews/master/chinaNews.zip\", \n","      extract=True)\n","  \n","  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                       \"chinaNews\", \"train\"))\n","  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                      \"chinaNews\", \"test\"))\n","  \n","  return train_df, test_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gAxDgyHTjlKE","colab_type":"code","colab":{}},"source":["train, test = download_and_load_datasets()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNn4AXWgjlO9","colab_type":"code","colab":{}},"source":["train = train.sample(11998)\n","test = test.sample(11998)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6IHPGOEDjlRx","colab_type":"code","colab":{}},"source":["train.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEsr7opHjlUX","colab_type":"code","colab":{}},"source":["DATA_COLUMN = 'sentence'\n","LABEL_COLUMN = 'polarity'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofzJMl_fj2uB","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgB2smMaj2wh","colab_type":"code","colab":{}},"source":["BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","def create_tokenizer_from_hub_module():\n","  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(BERT_MODEL_HUB)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","tokenizer = create_tokenizer_from_hub_module()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TtM9N_Krj2zL","colab_type":"code","colab":{}},"source":["tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FlUVKMkbj22d","colab_type":"code","colab":{}},"source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 380\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWXNSRltj24m","colab_type":"code","colab":{}},"source":["def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(\n","      BERT_MODEL_HUB,\n","      trainable=True)\n","  bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","  bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","\n","  hidden_size = output_layer.shape[-1].value\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabiltiies.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JscS3cZsj29f","colab_type":"code","colab":{}},"source":["# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predicted_labels)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predicted_labels) \n","        true_pos = tf.metrics.true_positives(\n","            label_ids,\n","            predicted_labels)\n","        true_neg = tf.metrics.true_negatives(\n","            label_ids,\n","            predicted_labels)   \n","        false_pos = tf.metrics.false_positives(\n","            label_ids,\n","            predicted_labels)  \n","        false_neg = tf.metrics.false_negatives(\n","            label_ids,\n","            predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-dIFJ_Nj3AD","colab_type":"code","colab":{}},"source":["# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","BATCH_SIZE = 32\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 3.0\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PzyIk6YkGcF","colab_type":"code","colab":{}},"source":["\n","# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z20TLk16kGfH","colab_type":"code","colab":{}},"source":["\n","# Specify outpit directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig(\n","    model_dir=OUTPUT_DIR,\n","    save_summary_steps=SAVE_SUMMARY_STEPS,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uI_SZW3bkGjz","colab_type":"code","colab":{}},"source":["model_fn = model_fn_builder(\n","  num_labels=len(label_list),\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps)\n","\n","estimator = tf.estimator.Estimator(\n","  model_fn=model_fn,\n","  config=run_config,\n","  params={\"batch_size\": BATCH_SIZE})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgjwPY-8kGiD","colab_type":"code","colab":{}},"source":["train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRCC_DFlkNPM","colab_type":"code","colab":{}},"source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WJnndhEEkNT9","colab_type":"code","colab":{}},"source":["test_input_fn = run_classifier.input_fn_builder(\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lehZGTT-kNXl","colab_type":"code","colab":{}},"source":["estimator.evaluate(input_fn=test_input_fn, steps=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdsn77fkj3Cw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Su4gxkrkj27d","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gD_Q17M2jlM2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYD2I6D2jlHf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JW4IhLUBkRly","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"wjSGl8oL0lzu","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"3vo2K5s1YPU5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jcw2N7d7YPOa","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pg623ugdYPML","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zz7pgIm0YH7J","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qq5k6TuYH44","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4M0QSRDlYHzs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}